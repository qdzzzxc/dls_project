{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Роман\",\n",
    "    \"Детектив\",\n",
    "    \"Фантастика\",\n",
    "    \"Приключения\",\n",
    "    \"Ужасы (Хоррор)\",\n",
    "    \"Фэнтези\",\n",
    "    \"Исторический роман\",\n",
    "    \"Триллер\",\n",
    "    \"Драма\",\n",
    "    \"Поэзия\",\n",
    "    \"Научно-популярная литература\",\n",
    "    \"Биография/автобиография\",\n",
    "    \"Философия\",\n",
    "    \"Психология\",\n",
    "    \"Эссе\",\n",
    "    \"Классика\",\n",
    "    \"Зарубежная классика\",\n",
    "    \"Публицистика\",\n",
    "    \"Фольклор\",\n",
    "    \"Комиксы/графические романы\",\n",
    "    \"Религиозная литература\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:13<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:04<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:13<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:04<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:12<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:13<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:15<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import tqdm \n",
    "\n",
    "links = []\n",
    "\n",
    "for name in names:\n",
    "    links.append(f'https://www.labirint.ru/search/{name}/?stype=1&page=')\n",
    "\n",
    "all_links = []\n",
    "\n",
    "for link in links:\n",
    "    for i in tqdm.tqdm(range(18)):\n",
    "        response = requests.get(link + str(i))\n",
    "\n",
    "        html_content = response.text\n",
    "\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        product_names = tree.xpath('//a[@class=\"product-card__name\"]')\n",
    "\n",
    "        for product_name in product_names:\n",
    "            href = product_name.get('href')\n",
    "            all_links.append('https://www.labirint.ru' + href)\n",
    "\n",
    "    print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pd_links \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mall_links\u001b[49m})\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[0;32m      4\u001b[0m pd_links\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks_to_books.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_links' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_links = pd.DataFrame({'link': all_links}).drop_duplicates()\n",
    "pd_links.to_csv('links_to_books.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-giant')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-giant')\n",
    "\n",
    "def embed_picture(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.detach().numpy()[0].flatten()\n",
    "    return  embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.labirint.ru/books/451006/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.labirint.ru/books/786953/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.labirint.ru/books/840898/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.labirint.ru/books/614463/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.labirint.ru/books/869738/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    link\n",
       "0  https://www.labirint.ru/books/451006/\n",
       "1  https://www.labirint.ru/books/786953/\n",
       "2  https://www.labirint.ru/books/840898/\n",
       "3  https://www.labirint.ru/books/614463/\n",
       "4  https://www.labirint.ru/books/869738/"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from lxml import html\n",
    "import tqdm \n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "pd_links = pd.read_csv('links_to_books.csv')\n",
    "pd_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        filename='parser.log',\n",
    "        filemode=\"w\",\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "        force=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1186 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1186/1186 [27:11<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "d = {'id': [], 'link': [], 'name': [], 'rate': [], 'description': [], 'reviews': []} #, 'embeddings': []}\n",
    "\n",
    "for index, row in tqdm.tqdm(pd_links[11_500:].iterrows(), total=len(pd_links[11_500:])):\n",
    "    try:\n",
    "        if index % 500 == 0:\n",
    "            books = pd.DataFrame(d) \n",
    "            books.to_csv('rows11_500-all.csv', index=False)\n",
    "\n",
    "        book_id = row.link.split('/')[-2]\n",
    "        response = requests.get(row.link, timeout=15)\n",
    "\n",
    "        html_content = response.text\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        name = tree.xpath(\"//div[@id='product-title']/h1\")[0].text\n",
    "        \n",
    "        rate = tree.xpath(\"//div[@id='rate']\")[0].text\n",
    "\n",
    "        descr = []\n",
    "        ps = tree.xpath(\"//div[@id='fullannotation']/p\")\n",
    "        if not ps:\n",
    "            ps = tree.xpath(\"//div[@id='fullannotation']/noindex/p\")\n",
    "        for p in ps:\n",
    "            descr.append(re.sub(r'\\xa0', ' ', p.text_content()))\n",
    "\n",
    "        descr = ' '.join(descr)\n",
    "\n",
    "        if not descr:\n",
    "            descr = tree.xpath(\"//div[@id='product-about']/p/noindex\")\n",
    "            if not descr:\n",
    "                descr = tree.xpath(\"//div[@id='product-about']/p\")\n",
    "            descr = descr[0].text_content()\n",
    "\n",
    "        reviews = []\n",
    "\n",
    "        for div in tree.xpath(\"//div[contains(@class, 'product-comment')]/div[4]/div/div[2]/noindex\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        for div in tree.xpath(\"//div[@class='comment-text content-comments']/div/p\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        reviews = '\\n;\\n'.join(reviews)\n",
    "\n",
    "        pic_url = tree.xpath(\"//div[@id='product-image']/img\")[0].get(\"data-src\")\n",
    "\n",
    "        # with open(f'pics/{book_id}.jpg', 'wb') as f:\n",
    "        #     response.raw.decode_content = True\n",
    "        #     shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "        img = Image.open(requests.get(pic_url, stream=True).raw)\n",
    "        img.save(f'pics/{book_id}.png')\n",
    "\n",
    "        # embeddings = embed_picture(img)\n",
    "\n",
    "        d['id'].append(book_id)\n",
    "        d['link'].append(row.link)\n",
    "        d['name'].append(name)\n",
    "        d['rate'].append(rate)\n",
    "        d['description'].append(descr)\n",
    "        d['reviews'].append(reviews)\n",
    "        \n",
    "        #d['embeddings'].append(embeddings)\n",
    "    except Exception:\n",
    "        logging.exception(f\"Exception occurred for link: {row.link}\")\n",
    "\n",
    "books = pd.DataFrame(d) \n",
    "books.to_csv('rows11_500-all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(d) \n",
    "books.to_csv('rows9_000-11_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/2000 [00:24<56:33,  1.71s/it] "
     ]
    }
   ],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "# driver = webdriver.Firefox()\n",
    "\n",
    "# d = {'id': [], 'link': [], 'name': [], 'rate': [], 'description': [], 'reviews': []} #, 'embeddings': []}\n",
    "\n",
    "# for index, row in tqdm.tqdm(pd_links[2_000:4_000].iterrows(), total=pd_links[2_000:4_000].shape[0]):\n",
    "#     try:\n",
    "#         book_id = row.link.split('/')[-2]\n",
    "        \n",
    "#         driver.get(row.link)\n",
    "\n",
    "#         name = driver.find_elements(By.XPATH, \"//div[@id='product-title']/h1\")[0].get_text()\n",
    "        \n",
    "#         rate = driver.find_elements(By.XPATH,\"//div[@id='rate']\")[0].get_text()\n",
    "\n",
    "#         descr = []\n",
    "#         ps = driver.find_elements(By.XPATH,\"//div[@id='fullannotation']/p\")\n",
    "#         if not ps:\n",
    "#             ps = driver.find_elements(By.XPATH,\"//div[@id='fullannotation']/noindex/p\")\n",
    "#         for p in ps:\n",
    "#             descr.append(re.sub(r'\\xa0', ' ', p.get_text()))\n",
    "\n",
    "#         descr = ' '.join(descr)\n",
    "\n",
    "#         if not descr:\n",
    "#             descr = driver.find_elements(By.XPATH,\"//div[@id='product-about']/p/noindex\")\n",
    "#             if not descr:\n",
    "#                 descr = driver.find_elements(By.XPATH,\"//div[@id='product-about']/p\")\n",
    "#             descr = descr[0].get_text()\n",
    "\n",
    "#         reviews = []\n",
    "\n",
    "#         for div in driver.find_elements(By.XPATH,\"//div[contains(@class, 'product-comment')]/div[4]/div/div[2]/noindex\"):\n",
    "#             reviews.append(div.get_text())\n",
    "\n",
    "#         for div in driver.find_elements(By.XPATH,\"//div[@class='comment-text content-comments']/div/p\"):\n",
    "#             reviews.append(div.get_text())\n",
    "\n",
    "#         reviews = '\\n;\\n'.join(reviews)\n",
    "\n",
    "#         pic_url = driver.find_elements(By.XPATH,\"//div[@id='product-image']/img\")[0].get(\"data-src\")\n",
    "\n",
    "#         #response = requests.get(pic_url)\n",
    "\n",
    "#         # with open(f'pics/{book_id}.jpg', 'wb') as f:\n",
    "#         #     response.raw.decode_content = True\n",
    "#         #     shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "#         # img = Image.open(requests.get(pic_url, stream=True).raw)\n",
    "#         # img.save(f'pics/{book_id}.png')\n",
    "\n",
    "#         # embeddings = embed_picture(img)\n",
    "\n",
    "#         d['id'].append(book_id)\n",
    "#         d['link'].append(row.link)\n",
    "#         d['name'].append(name)\n",
    "#         d['rate'].append(rate)\n",
    "#         d['description'].append(descr)\n",
    "#         d['reviews'].append(reviews)\n",
    "        \n",
    "#         #d['embeddings'].append(embeddings)\n",
    "#     except Exception:\n",
    "#         logging.exception(f\"Exception occurred for link: {row.link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(d) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_csv('rows7_000-9_000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1731/2000 [35:38<05:18,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "d = {'id': [], 'link': [], 'name': [], 'rate': [], 'description': [], 'reviews': []} #, 'embeddings': []}\n",
    "\n",
    "for index, row in tqdm.tqdm(pd_links[6_000:8_000].iterrows(), total=pd_links[6_000:8_000].shape[0]):\n",
    "    try:\n",
    "        book_id = row.link.split('/')[-2]\n",
    "        response = requests.get(row.link)\n",
    "\n",
    "        html_content = response.text\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        name = tree.xpath(\"//div[@id='product-title']/h1\")[0].text\n",
    "        \n",
    "        rate = tree.xpath(\"//div[@id='rate']\")[0].text\n",
    "\n",
    "        descr = []\n",
    "        ps = tree.xpath(\"//div[@id='fullannotation']/p\")\n",
    "        if not ps:\n",
    "            ps = tree.xpath(\"//div[@id='fullannotation']/noindex/p\")\n",
    "        for p in ps:\n",
    "            descr.append(re.sub(r'\\xa0', ' ', p.text_content()))\n",
    "\n",
    "        descr = ' '.join(descr)\n",
    "\n",
    "        if not descr:\n",
    "            descr = tree.xpath(\"//div[@id='product-about']/p/noindex\")\n",
    "            if not descr:\n",
    "                descr = tree.xpath(\"//div[@id='product-about']/p\")\n",
    "            descr = descr[0].text_content()\n",
    "\n",
    "        reviews = []\n",
    "\n",
    "        for div in tree.xpath(\"//div[contains(@class, 'product-comment')]/div[4]/div/div[2]/noindex\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        for div in tree.xpath(\"//div[@class='comment-text content-comments']/div/p\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        reviews = '\\n;\\n'.join(reviews)\n",
    "\n",
    "        pic_url = tree.xpath(\"//div[@id='product-image']/img\")[0].get(\"data-src\")\n",
    "\n",
    "        response = requests.get(pic_url)\n",
    "\n",
    "        # with open(f'pics/{book_id}.jpg', 'wb') as f:\n",
    "        #     response.raw.decode_content = True\n",
    "        #     shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "        # img = Image.open(requests.get(pic_url, stream=True).raw)\n",
    "        # img.save(f'pics/{book_id}.png')\n",
    "\n",
    "        # embeddings = embed_picture(img)\n",
    "\n",
    "        d['id'].append(book_id)\n",
    "        d['link'].append(row.link)\n",
    "        d['name'].append(name)\n",
    "        d['rate'].append(rate)\n",
    "        d['description'].append(descr)\n",
    "        d['reviews'].append(reviews)\n",
    "        \n",
    "        #d['embeddings'].append(embeddings)\n",
    "    except Exception:\n",
    "        logging.exception(f\"Exception occurred for link: {row.link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(d) \n",
    "books.to_csv('rows6_000-8_000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': [], 'link': [], 'name': [], 'rate': [], 'description': [], 'reviews': []} #, 'embeddings': []}\n",
    "\n",
    "for index, row in tqdm.tqdm(pd_links[8_000:10_000].iterrows(), total=pd_links[8_000:10_000].shape[0]):\n",
    "    try:\n",
    "        book_id = row.link.split('/')[-2]\n",
    "        response = requests.get(row.link)\n",
    "\n",
    "        html_content = response.text\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        name = tree.xpath(\"//div[@id='product-title']/h1\")[0].text\n",
    "        \n",
    "        rate = tree.xpath(\"//div[@id='rate']\")[0].text\n",
    "\n",
    "        descr = []\n",
    "        ps = tree.xpath(\"//div[@id='fullannotation']/p\")\n",
    "        if not ps:\n",
    "            ps = tree.xpath(\"//div[@id='fullannotation']/noindex/p\")\n",
    "        for p in ps:\n",
    "            descr.append(re.sub(r'\\xa0', ' ', p.text_content()))\n",
    "\n",
    "        descr = ' '.join(descr)\n",
    "\n",
    "        if not descr:\n",
    "            descr = tree.xpath(\"//div[@id='product-about']/p/noindex\")\n",
    "            if not descr:\n",
    "                descr = tree.xpath(\"//div[@id='product-about']/p\")\n",
    "            descr = descr[0].text_content()\n",
    "\n",
    "        reviews = []\n",
    "\n",
    "        for div in tree.xpath(\"//div[contains(@class, 'product-comment')]/div[4]/div/div[2]/noindex\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        for div in tree.xpath(\"//div[@class='comment-text content-comments']/div/p\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        reviews = '\\n;\\n'.join(reviews)\n",
    "\n",
    "        pic_url = tree.xpath(\"//div[@id='product-image']/img\")[0].get(\"data-src\")\n",
    "\n",
    "        response = requests.get(pic_url)\n",
    "\n",
    "        # with open(f'pics/{book_id}.jpg', 'wb') as f:\n",
    "        #     response.raw.decode_content = True\n",
    "        #     shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "        # img = Image.open(requests.get(pic_url, stream=True).raw)\n",
    "        # img.save(f'pics/{book_id}.png')\n",
    "\n",
    "        # embeddings = embed_picture(img)\n",
    "\n",
    "        d['id'].append(book_id)\n",
    "        d['link'].append(row.link)\n",
    "        d['name'].append(name)\n",
    "        d['rate'].append(rate)\n",
    "        d['description'].append(descr)\n",
    "        d['reviews'].append(reviews)\n",
    "        \n",
    "        #d['embeddings'].append(embeddings)\n",
    "    except Exception:\n",
    "        logging.exception(f\"Exception occurred for link: {row.link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(d) \n",
    "books.to_csv('rows8_000-10_000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id': [], 'link': [], 'name': [], 'rate': [], 'description': [], 'reviews': []} #, 'embeddings': []}\n",
    "\n",
    "for index, row in tqdm.tqdm(pd_links[10_000:12_000].iterrows(), total=pd_links[10_000:12_000].shape[0]):\n",
    "    try:\n",
    "        book_id = row.link.split('/')[-2]\n",
    "        response = requests.get(row.link)\n",
    "\n",
    "        html_content = response.text\n",
    "        tree = html.fromstring(html_content)\n",
    "\n",
    "        name = tree.xpath(\"//div[@id='product-title']/h1\")[0].text\n",
    "        \n",
    "        rate = tree.xpath(\"//div[@id='rate']\")[0].text\n",
    "\n",
    "        descr = []\n",
    "        ps = tree.xpath(\"//div[@id='fullannotation']/p\")\n",
    "        if not ps:\n",
    "            ps = tree.xpath(\"//div[@id='fullannotation']/noindex/p\")\n",
    "        for p in ps:\n",
    "            descr.append(re.sub(r'\\xa0', ' ', p.text_content()))\n",
    "\n",
    "        descr = ' '.join(descr)\n",
    "\n",
    "        if not descr:\n",
    "            descr = tree.xpath(\"//div[@id='product-about']/p/noindex\")\n",
    "            if not descr:\n",
    "                descr = tree.xpath(\"//div[@id='product-about']/p\")\n",
    "            descr = descr[0].text_content()\n",
    "\n",
    "        reviews = []\n",
    "\n",
    "        for div in tree.xpath(\"//div[contains(@class, 'product-comment')]/div[4]/div/div[2]/noindex\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        for div in tree.xpath(\"//div[@class='comment-text content-comments']/div/p\"):\n",
    "            reviews.append(div.text_content())\n",
    "\n",
    "        reviews = '\\n;\\n'.join(reviews)\n",
    "\n",
    "        pic_url = tree.xpath(\"//div[@id='product-image']/img\")[0].get(\"data-src\")\n",
    "\n",
    "        response = requests.get(pic_url)\n",
    "\n",
    "        # with open(f'pics/{book_id}.jpg', 'wb') as f:\n",
    "        #     response.raw.decode_content = True\n",
    "        #     shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "        # img = Image.open(requests.get(pic_url, stream=True).raw)\n",
    "        # img.save(f'pics/{book_id}.png')\n",
    "\n",
    "        # embeddings = embed_picture(img)\n",
    "\n",
    "        d['id'].append(book_id)\n",
    "        d['link'].append(row.link)\n",
    "        d['name'].append(name)\n",
    "        d['rate'].append(rate)\n",
    "        d['description'].append(descr)\n",
    "        d['reviews'].append(reviews)\n",
    "        \n",
    "        #d['embeddings'].append(embeddings)\n",
    "    except Exception:\n",
    "        logging.exception(f\"Exception occurred for link: {row.link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(d) \n",
    "books.to_csv('rows10_000-12_000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (9.0.1)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\никита\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Load image, convert to HSV format, define lower/upper ranges, and perform\n",
    "# color segmentation to create a binary mask\n",
    "image = cv2.imread(r'pics\\1008835.png')\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "lower = np.array([0, 0, 200])\n",
    "upper = np.array([255, 255, 255])\n",
    "mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "# Create horizontal kernel and dilate to connect text characters\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,3))\n",
    "dilate = cv2.dilate(mask, kernel, iterations=5)\n",
    "\n",
    "# Find contours and filter using aspect ratio\n",
    "# Remove non-text contours by filling in the contour\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for c in cnts:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    ar = w / float(h)\n",
    "    if ar < 5:\n",
    "        cv2.drawContours(dilate, [c], -1, (0,0,0), -1)\n",
    "\n",
    "# Bitwise dilated image with mask, invert, then OCR\n",
    "result = 255 - cv2.bitwise_and(dilate, mask)\n",
    "\n",
    "#data = pytesseract.image_to_boxes(mask, lang='rus',config='--psm 6', output_type=Output.DICT)\n",
    "\n",
    "cv2.imshow('mask', mask)\n",
    "cv2.imshow('dilate', dilate)\n",
    "cv2.imshow('result', result)\n",
    "cv2.waitKey()\n",
    "\n",
    "# n_boxes = len(data['char'])\n",
    "# for i in range(n_boxes):\n",
    "#     print(data['left'])\n",
    "#     (x, y, w, h) = (data['left'][i], data['top'][i], data['right'][i] - data['left'][i], data['top'][i] - data['bottom'][i])\n",
    "\n",
    "#     cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#     print((x, y), (x + w, y + h))\n",
    "\n",
    "\n",
    "# cv2.imshow('img', image)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char': ['~'], 'left': [0], 'bottom': [0], 'right': [360], 'top': [561], 'page': [0]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'level'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m d \u001b[38;5;241m=\u001b[39m pytesseract\u001b[38;5;241m.\u001b[39mimage_to_boxes(mask, output_type\u001b[38;5;241m=\u001b[39mOutput\u001b[38;5;241m.\u001b[39mDICT)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n\u001b[1;32m---> 15\u001b[0m n_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlevel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_boxes):\n\u001b[0;32m     17\u001b[0m     (x, y, w, h) \u001b[38;5;241m=\u001b[39m (d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m][i], d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m'\u001b[39m][i], d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m][i], d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'level'"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "\n",
    "# img = cv2.imread(r'pics\\1010687.png')\n",
    "# img = cv2.imread(r'pics\\1010326.png')\n",
    "img = cv2.imread(r'pics\\1008835.png')\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "lower = np.array([0, 0, 100])\n",
    "upper = np.array([255, 255, 255])\n",
    "mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "d = pytesseract.image_to_boxes(img, output_type=Output.DICT)\n",
    "print(d)\n",
    "n_boxes = len(d['level'])\n",
    "for i in range(n_boxes):\n",
    "    (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n",
    "    if d['conf'][i] > 0:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5],\n",
       " 'page_num': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'block_num': [0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4],\n",
       " 'par_num': [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'line_num': [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1],\n",
       " 'word_num': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " 'left': [0,\n",
       "  82,\n",
       "  82,\n",
       "  82,\n",
       "  82,\n",
       "  205,\n",
       "  92,\n",
       "  92,\n",
       "  204,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  37,\n",
       "  37,\n",
       "  37,\n",
       "  33,\n",
       "  86,\n",
       "  131,\n",
       "  156,\n",
       "  212,\n",
       "  248,\n",
       "  38,\n",
       "  38,\n",
       "  146,\n",
       "  208,\n",
       "  139,\n",
       "  139,\n",
       "  139,\n",
       "  139],\n",
       " 'top': [0,\n",
       "  38,\n",
       "  38,\n",
       "  38,\n",
       "  38,\n",
       "  38,\n",
       "  64,\n",
       "  64,\n",
       "  64,\n",
       "  119,\n",
       "  119,\n",
       "  119,\n",
       "  119,\n",
       "  490,\n",
       "  490,\n",
       "  490,\n",
       "  478,\n",
       "  478,\n",
       "  478,\n",
       "  478,\n",
       "  478,\n",
       "  491,\n",
       "  500,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  302,\n",
       "  302,\n",
       "  302,\n",
       "  302],\n",
       " 'width': [363,\n",
       "  216,\n",
       "  216,\n",
       "  216,\n",
       "  102,\n",
       "  93,\n",
       "  196,\n",
       "  89,\n",
       "  84,\n",
       "  314,\n",
       "  314,\n",
       "  314,\n",
       "  314,\n",
       "  233,\n",
       "  233,\n",
       "  233,\n",
       "  47,\n",
       "  41,\n",
       "  17,\n",
       "  52,\n",
       "  33,\n",
       "  22,\n",
       "  194,\n",
       "  104,\n",
       "  59,\n",
       "  24,\n",
       "  107,\n",
       "  107,\n",
       "  107,\n",
       "  107],\n",
       " 'height': [550,\n",
       "  37,\n",
       "  37,\n",
       "  13,\n",
       "  11,\n",
       "  13,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  19,\n",
       "  19,\n",
       "  8,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  5,\n",
       "  9,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  172,\n",
       "  172,\n",
       "  172,\n",
       "  172],\n",
       " 'conf': [-1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  86,\n",
       "  55,\n",
       "  -1,\n",
       "  15,\n",
       "  90,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  37,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  26,\n",
       "  2,\n",
       "  17,\n",
       "  2,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  38,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  95],\n",
       " 'text': ['',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'POBEPT',\n",
       "  'AAHUA',\n",
       "  '',\n",
       "  'H3IHCH',\n",
       "  'KPECC',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'HABANAIOALATCAD',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '“Axyonsine',\n",
       "  'ocraenen',\n",
       "  'roy',\n",
       "  'prapouscre',\n",
       "  '200-10',\n",
       "  'one',\n",
       "  '',\n",
       "  '‘SymcnoeNetprsouad',\n",
       "  'sonetunootns',\n",
       "  'pou',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(image).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(mask).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(dilate).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(result).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 363)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('result', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "1+ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
